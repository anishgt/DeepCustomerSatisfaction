head(autcostlist)
testdata1=testdata
testdata1$cost = as.numeric(autcostlist)
head(testdata1)
which(testdata1$cost>0, arr.ind = TRUE)
max(testdata1$cost)
ind=which(testdata1$cost==1, arr.ind = TRUE)
max(testdata$cost[-ind])
testdata$cost[1]
testdata$cost[,1]
testdata$cost[1,]
testdata$Id[1]
testdata1=testdata
testdata1$cost = as.numeric(unlist(autcostlist))
testdata$cost[1]
autcostlist = rep(0,nrow(testdata))
autcostlist
for(i in uni){
#authorid = i
#authoridCost = author.mat[i,2]
authoridCost=as.numeric(author.list[[i]][2])
autcostlist[which(testdata$Id == i, arr.ind = TRUE)]=authoridCost
}
autcostlist
testdata1= cbind(testdata, autcostlist)
head(testdata1)
testdata$cost[1]
testdata$autcostlist[1]
testdata1= cbind(testdata,cost= autcostlist)
head(testdata)
head(testdata1)
testdata1$autcostlist[1]
testdata1$cost[1]
ind=which(testdata1$cost==1, arr.ind = TRUE)
max(testdata$cost[-ind])
max(testdata1$cost[-ind])
mydata = read.csv("/media/pragati/New Volume/ssrivas6/MLProject/Output.csv", sep="|")
mydata = read.csv("/media/pragati/New Volume/ssrivas6/MLProject/Output.csv", sep="|")
mydata = read.csv("/media/pragati/New Volume/ssrivas6/MLProject/Output.csv", sep="|")
mynames=c("Restaurant", "RestaurantID", "AvgRating","AuthorName","Id","NumberOfFriends", "NumberOfReviews","Rating", "Review", "Date", "Class")
names(mydata)=mynames
testdata<-mydata[sample(nrow(mydata), size=5000, replace=FALSE),]
tdata = as.data.frame(testdata$Review)
docs <- Corpus(DataframeSource(tdata))
#docs[[1]]$content
# remove punctuations
docs <- tm_map(docs, removePunctuation)
# remove special characters
for(j in seq(docs))
{
docs[[j]] <- gsub("/", " ", docs[[j]])
docs[[j]] <- gsub("@", " ", docs[[j]])
docs[[j]] <- gsub("\\|", " ", docs[[j]])
docs[[j]] <- gsub("#", " ", docs[[j]])
docs[[j]] <- gsub("&", " ", docs[[j]])
docs[[j]] <- gsub("$", " ", docs[[j]])
}
# inspect(docs[1])
#Removing numbers:
docs <- tm_map(docs, removeNumbers)
#Converting to lowercase:
docs <- tm_map(docs, tolower)
# Removing “stopwords” :
docs <- tm_map(docs, removeWords, stopwords("english"))
#writings <- tm_map(writings, removeWords, stopwords(“SMART”))
#Removing common word endings (e.g., “ing”, “es”, “s”)
library(SnowballC)
docs <- tm_map(docs, stemDocument)
#Stripping unnecesary whitespace
docs <- tm_map(docs, stripWhitespace)
#end of the preprocessing
docs <- tm_map(docs, PlainTextDocument)
#create a document term matrix
dtm <- DocumentTermMatrix(docs)
#inspect(dtm[1:5, 1:20])
#transpose
tdm <- TermDocumentMatrix(docs)
#Organize terms by their frequency:
freq <- colSums(as.matrix(dtm))
#length(freq)
ord <- order(freq)
# removing sparse terms:
###dtms <- removeSparseTerms(dtm, 0.01) # This makes a matrix that is 10% empty space, maximum.
#inspect(dtms)
#frequecy
#freq <- colSums(as.matrix(dtm))
#freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
#head(freq, 14)
freq <- colSums(as.matrix(dtm))
freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
#head(freq, 14)
wf <- data.frame(word=names(freq), freq=freq)
library(tm)
testdata<-mydata[sample(nrow(mydata), size=5000, replace=FALSE),]
tdata = as.data.frame(testdata$Review)
docs <- Corpus(DataframeSource(tdata))
docs <- tm_map(docs, removePunctuation)
# remove special characters
for(j in seq(docs))
{
docs[[j]] <- gsub("/", " ", docs[[j]])
docs[[j]] <- gsub("@", " ", docs[[j]])
docs[[j]] <- gsub("\\|", " ", docs[[j]])
docs[[j]] <- gsub("#", " ", docs[[j]])
docs[[j]] <- gsub("&", " ", docs[[j]])
docs[[j]] <- gsub("$", " ", docs[[j]])
}
# inspect(docs[1])
#Removing numbers:
docs <- tm_map(docs, removeNumbers)
#Converting to lowercase:
docs <- tm_map(docs, tolower)
# Removing “stopwords” :
docs <- tm_map(docs, removeWords, stopwords("english"))
library(SnowballC)
docs <- tm_map(docs, stemDocument)
#Stripping unnecesary whitespace
docs <- tm_map(docs, stripWhitespace)
#end of the preprocessing
docs <- tm_map(docs, PlainTextDocument)
#create a document term matrix
dtm <- DocumentTermMatrix(docs)
#inspect(dtm[1:5, 1:20])
#transpose
tdm <- TermDocumentMatrix(docs)
#Organize terms by their frequency:
freq <- colSums(as.matrix(dtm))
#length(freq)
ord <- order(freq)
# removing sparse terms:
###dtms <- removeSparseTerms(dtm, 0.01) # This makes a matrix that is 10% empty space, maximum.
#inspect(dtms)
#frequecy
#freq <- colSums(as.matrix(dtm))
#freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
#head(freq, 14)
mydata = read.csv("/media/pragati/New Volume/ssrivas6/MLProject/Output.csv", sep="|")
mynames=c("Restaurant", "RestaurantID", "AvgRating","AuthorName","Id","NumberOfFriends", "NumberOfReviews","Rating", "Review", "Date", "Class")
names(mydata)=mynames
library(tm)
testdata<-mydata[sample(nrow(mydata), size=5000, replace=FALSE),]
testdata=subset(testdata, Class == 1)
tdata = as.data.frame(testdata$Review)
testdata<-mydata[sample(nrow(mydata), size=5000, replace=FALSE),]
tdata = as.data.frame(testdata$Review)
mydata = read.csv("/media/pragati/New Volume/ssrivas6/MLProject/Output.csv", sep="|")
mydata = read.csv("/media/pragati/New Volume/ssrivas6/MLProject/Output.csv", sep="|")
mynames=c("Restaurant", "RestaurantID", "AvgRating","AuthorName","Id","NumberOfFriends", "NumberOfReviews","Rating", "Review", "Date", "Class")
names(mydata)=mynames
library(tm)
testdata=mydata
tdata = as.data.frame(testdata$Review)
docs <- Corpus(DataframeSource(tdata))
docs <- tm_map(docs, removePunctuation)
# remove special characters
for(j in seq(docs))
{
docs[[j]] <- gsub("/", " ", docs[[j]])
docs[[j]] <- gsub("@", " ", docs[[j]])
docs[[j]] <- gsub("\\|", " ", docs[[j]])
docs[[j]] <- gsub("#", " ", docs[[j]])
docs[[j]] <- gsub("&", " ", docs[[j]])
docs[[j]] <- gsub("$", " ", docs[[j]])
}
nrow(testdata)
summary(testdata)
testdata=unique(testdata)
nrow(testdata)
mydata = read.csv("/media/pragati/New Volume/ssrivas6/MLProject/Output.csv", sep="|")
mydata=unique(mydata)
mynames=c("Restaurant", "RestaurantID", "AvgRating","AuthorName","Id","NumberOfFriends", "NumberOfReviews","Rating", "Review", "Date", "Class")
names(mydata)=mynames
library(tm)
testdata<-mydata[sample(nrow(mydata), size=5000, replace=FALSE),]
#testdata=subset(testdata, Class == 1)
tdata = testdata$Review
tdata = as.data.frame(testdata$Review)
docs <- Corpus(DataframeSource(tdata))
#docs[[1]]$content
# remove punctuations
docs <- tm_map(docs, removePunctuation)
# remove special characters
for(j in seq(docs))
{
docs[[j]] <- gsub("/", " ", docs[[j]])
docs[[j]] <- gsub("@", " ", docs[[j]])
docs[[j]] <- gsub("\\|", " ", docs[[j]])
docs[[j]] <- gsub("#", " ", docs[[j]])
docs[[j]] <- gsub("&", " ", docs[[j]])
docs[[j]] <- gsub("$", " ", docs[[j]])
}
docs <- tm_map(docs, removeNumbers)
#Converting to lowercase:
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, removeWords, stopwords("english"))
#writings <- tm_map(writings, removeWords, stopwords(“SMART”))
#Removing common word endings (e.g., “ing”, “es”, “s”)
library(SnowballC)
docs <- tm_map(docs, stemDocument)
docs <- tm_map(docs, stripWhitespace)
#end of the preprocessing
docs <- tm_map(docs, PlainTextDocument)
#create a document term matrix
dtm <- DocumentTermMatrix(docs)
#inspect(dtm[1:5, 1:20])
#transpose
tdm <- TermDocumentMatrix(docs)
#Organize terms by their frequency:
freq <- colSums(as.matrix(dtm))
#length(freq)
ord <- order(freq)
# removing sparse terms:
dtms <- removeSparseTerms(dtm, 0.1) # This makes a matrix that is 10% empty space, maximum.
#inspect(dtms)
#frequecy
freq <- colSums(as.matrix(dtm))
freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
#head(freq, 14)
wf <- data.frame(word=names(freq), freq=freq)
freq <- colSums(as.matrix(dtms))
freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
#head(freq, 14)
#Plot
library(ggplot2)
p <- ggplot(subset(wf, freq>2000), aes(word, freq))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
p
#Cloud Word
library(wordcloud)
set.seed(142)
dark2 <- brewer.pal(6, "Dark2")
wordcloud(names(freq), freq, max.words=100, rot.per=0.2, colors=dark2)
library(lsa)
ll = list()
uni = as.character(unique(unlist(testdata$Id)))
for(i in seq(1:length(uni))){
author = uni[i]
author.index = which(testdata$Id == author, arr.ind = TRUE)
if(length(author.index) > 1){
author.review = docs[author.index]
dtmr <- DocumentTermMatrix(author.review)
dtm.m <- as.matrix(dtmr)
#d <- dist(dtm.m, method = "euclidean")
dtm.t <- t(dtm.m)
d <- cosine(dtm.t)
diag(d) = 0
ll=c(ll,max(d))
}else{
ll=c(ll,0)
}
}
author.list = mapply(c,uni , ll, SIMPLIFY=FALSE)
#author.mat = do.call(rbind, author.list)
autcostlist = rep(0,nrow(testdata))
#for(i in seq(1:nrow(author.mat))){
for(i in uni){
#authorid = i
#authoridCost = author.mat[i,2]
authoridCost=as.numeric(author.list[[i]][2])
autcostlist[which(testdata$Id == i, arr.ind = TRUE)]=authoridCost
}
testdata1= cbind(testdata,cost= autcostlist)
#c=which(testdata1$cost>0, arr.ind = TRUE)
ind=which(testdata1$cost==1, arr.ind = TRUE)
max(testdata1$cost[-ind])
mydata = read.csv("/media/pragati/New Volume/ssrivas6/MLProject/Output.csv", sep="|")
mydata = read.csv("/media/pragati/New Volume/ssrivas6/MLProject/Output.csv", sep="|")
mydata=unique(mydata)
mynames=c("Restaurant", "RestaurantID", "AvgRating","AuthorName","Id","NumberOfFriends", "NumberOfReviews","Rating", "Review", "Date", "Class")
names(mydata)=mynames
mydata = read.csv("/media/pragati/New Volume/ssrivas6/MLProject/OutputBR.csv", sep="|")
mydata = read.csv("/media/pragati/New Volume/ssrivas6/MLProject/Output.csv", sep="|")
mydata=unique(mydata)
mynames=c("Restaurant", "RestaurantID", "AvgRating","AuthorName","Id","NumberOfFriends", "NumberOfReviews","Rating", "Review", "Date", "Class")
names(mydata)=mynames
names(mydata)=mynames
head(mydata)
fakedata = = read.csv("/media/pragati/New Volume/ssrivas6/MLProject/OutputBR.csv", sep="|")
fakedata = read.csv("/media/pragati/New Volume/ssrivas6/MLProject/OutputBR.csv", sep="|")
head(fakedata)
uni = as.character(unique(fakedata$RestaurantID))
fakedata=unique(fakedata)
nrow(fakedata)
uni = as.character(unique(fakedata$RestaurantID))
for(i in uni){
indx=which(mydata$RestaurantID==i)[1]
res.name=as.character(mydata$Restaurant[indx])
res.rating=as.numeric(mydata$AvgRating[indx])
indexes=which(fakedata$RestaurantID==i)
for(x in indexes){
fakedata$Restaurant[x]=res.name
fakedata$AvgRating[x]=res.rating
}
}
nrow(fakedata)
head(fakedata)
mydata = read.csv("/media/pragati/New Volume/ssrivas6/MLProject/Output.csv", sep="|")
mydata=unique(mydata)
mynames=c("Restaurant", "RestaurantID", "AvgRating","AuthorName","Id","NumberOfFriends", "NumberOfReviews","Rating", "Review", "Date", "Class")
names(mydata)=mynames
fakedata = read.csv("/media/pragati/New Volume/ssrivas6/MLProject/OutputBR.csv", sep="|")
names(fakedata)=mynames
uni = as.character(unique(fakedata$RestaurantID))
fakedata=unique(fakedata)
uni = as.character(unique(fakedata$RestaurantID))
for(i in uni){
indx=which(mydata$RestaurantID==i)[1]
res.name=as.character(mydata$Restaurant[indx])
res.rating=as.numeric(mydata$AvgRating[indx])
indexes=which(fakedata$RestaurantID==i)
for(x in indexes){
fakedata$Restaurant[x]=res.name
fakedata$AvgRating[x]=res.rating
}
}
nrow(fakedata)
head(fakedata)
nrow(mydata)
mydata=rbind(mydata,fakedata)
nrow(mydata)
tail(mydata)
mydata = read.csv("/media/pragati/New Volume/ssrivas6/MLProject/Output.csv", sep="|")
mydata = read.csv("/media/pragati/New Volume/ssrivas6/MLProject/Output.csv", sep="|")
mydata=unique(mydata)
mynames=c("Restaurant", "RestaurantID", "AvgRating","AuthorName","Id","NumberOfFriends", "NumberOfReviews","Rating", "Review", "Date", "Class")
names(mydata)=mynames
fakedata = read.csv("/media/pragati/New Volume/ssrivas6/MLProject/OutputBR.csv", sep="|")
names(fakedata)=mynames
fakedata=unique(fakedata)
uni = as.character(unique(fakedata$RestaurantID))
for(i in uni){
indx=which(mydata$RestaurantID==i)[1]
res.name=as.character(mydata$Restaurant[indx])
res.rating=as.numeric(mydata$AvgRating[indx])
indexes=which(fakedata$RestaurantID==i)
for(x in indexes){
fakedata$Restaurant[x]=res.name
fakedata$AvgRating[x]=res.rating
}
}
nrow(mydata)
nrow(fakedata)
mydata=rbind(mydata,fakedata)
length(mydata)
length(unique(mydata))
nrow(unique(mydata))
nrow(mydata)
mydata=unique(mydata)
nrow(mydata)
mydata$Date=regmatches(mydata$Date,gregexpr("\\d{1,2}/\\d{1,2}/\\d{4}",mydata$Date,perl=TRUE))
nrow(mydata)
summary(mydata)
head(mydata)
class(mydata$Date)
class(mydata$RestaurantID)
a= fakedata[!grepl("This review has been removed for violating our Terms of Service", fakedata$Review),]
a
b= fakedata[grepl("This review has been removed for violating our Terms of Service", fakedata$Review),]
head(b)
nrow(b)
mydata= mydata[!grepl("This review has been removed for violating our Terms of Service", mydata$Review),]
nrow(mydata)
write.csv(mydata, file = "mydatafinal.csv",row.names=FALSE, na="")
mydata = read.csv("/media/pragati/New Volume/ssrivas6/MLProject/Output.csv", sep="|")
mydata=unique(mydata)
mynames=c("Restaurant", "RestaurantID", "AvgRating","AuthorName","Id","NumberOfFriends", "NumberOfReviews","Rating", "Review", "Date", "Class")
names(mydata)=mynames
fakedata = read.csv("/media/pragati/New Volume/ssrivas6/MLProject/OutputBR.csv", sep="|")
names(fakedata)=mynames
fakedata=unique(fakedata)
uni = as.character(unique(fakedata$RestaurantID))
for(i in uni){
indx=which(mydata$RestaurantID==i)[1]
res.name=as.character(mydata$Restaurant[indx])
res.rating=as.numeric(mydata$AvgRating[indx])
indexes=which(fakedata$RestaurantID==i)
for(x in indexes){
fakedata$Restaurant[x]=res.name
fakedata$AvgRating[x]=res.rating
}
}
mydata=rbind(mydata,fakedata)
mydata=unique(mydata)
nrow(mydata)
mydata= mydata[!grepl("This review has been removed for violating our Terms of Service", mydata$Review),]
nrow(mydata)
write.csv(mydata, file = "/media/pragati/New Volume/ssrivas6/MLProject/MyDataFinal.csv",row.names=FALSE, na="")
nrow(mydata)
mydata$Date=regmatches(mydata$Date,gregexpr("\\d{1,2}/\\d{1,2}/\\d{4}",mydata$Date,perl=TRUE))
nrow(mydata)
mydatafinal = read.csv("/media/pragati/New Volume/ssrivas6/MLProject/MyDataFinal.csv")
head(mydatafinal)
summary(mydatafinal)
nrow(unique((mydatafinal))
)
nrow(mydatafinal)
q()
getwd()
clust=read.table("4.clusters", header=F, sep=";")
clust=read.table("4.clusters.txt", header=F, sep=";")
clust=read.table("4.clusters.txt", header=F, sep=";")
clust
clust=read.table("4.clusters", header=F)
clust
clust=read.table("4.clusters", header=F, sep="\n")
clust
clust[1]
class(clust)
clust$V1[1]
clust$V1[2]
clust$V1[6]
vec.new=rep(0, 480)
length(vec.new)
clusters=1
for (i in clust$V1){
if(length(i)==1){
vec.new[i]=0
} else {
for( j in i){
vec.new[j]=clusters
}
clusters=cluster+1
}
}
vec.new
class(vec.new)
vec.new=rep(0, 480)
clusters=1
for (i in clust$V1){
if(length(i)==1){
vec.new[as.numeric(i)]=0
} else {
for( j in i){
vec.new[as.numeric(j)]=clusters
}
clusters=cluster+1
}
}
vec.new
class(clust$V1)
clusters=1
for (i in as.vector(clust$V1)){
if(length(i)==1){
vec.new[as.numeric(i)]=0
} else {
for( j in i){
vec.new[as.numeric(j)]=clusters
}
clusters=cluster+1
}
}
vec.new
class(clust$V1[6])
clust$V1[6]
strsplit(clust$V1[6], ",")
strsplit(as.string(clust$V1[6]), ",")
strsplit(as.character(clust$V1[6]), ",")
as.vector(as.numeric(strsplit(as.character(clust$V1[6]), ",")))
as.vector(strsplit(as.character(clust$V1[6]), ","))
clusters=1
for (k in clust$V1){
i=(strsplit(as.character(k), ",")[[1]]
if(length(i)==1){
vec.new[as.numeric(i)]=0
} else {
for( j in i){
vec.new[as.numeric(j)]=clusters
}
clusters=cluster+1
}
}
clusters=1
for (k in clust$V1){
i=(strsplit(as.character(k), ","))[[1]]
if(length(i)==1){
vec.new[as.numeric(i)]=0
} else {
for( j in i){
vec.new[as.numeric(j)]=clusters
}
clusters=clusters+1
}
}
vec.new
clusters=1
for (k in clust$V1){
i=(strsplit(as.character(k), ","))[[1]]
if(length(i)==1){
vec.new[as.numeric(i)+1]=0
} else {
for( j in i){
vec.new[as.numeric(j)+1]=clusters
}
clusters=clusters+1
}
}
vec.new
q()
getwd()
train=read.csv("train.csv", header=T)
setwd("~/Git_Pragati/DeepCustomerSatisfaction/santander")
train=read.csv("train.csv", header=T)
train_sub=train[,c('var15', 'ind_var5', 'ind_var8_0', 'ind_var30', 'num_var5', 'num_var30', 'num_var42', 'var36', 'num_meses_var5_ult3')]
head(train_sub)
unique(train_sub$num_var5)
unique(train_sub$num_var30)
unique(train_sub$num_var42)
unique(train_sub$var15)
unique(train_sub$var36)
unique(train_sub$num_meses_var5_ult3)
q()
